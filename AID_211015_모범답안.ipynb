{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AID_211015_모범답안.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOZO35MrqW8ToCnIzqsePum",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiunny/AID/blob/main/AID_211015_%EB%AA%A8%EB%B2%94%EB%8B%B5%EC%95%88.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEUa-kLq-P6m"
      },
      "source": [
        "# 실습 1\n",
        "## 1. 다음의 의사결정 규칙을 결정 트리로 표현하여 보시오.\n",
        "\n",
        "- 날씨가 흐리면 집안에 있는다.\n",
        "- 날씨가 맑고 기온이 25도 미만인 경우는 등산을 한다.\n",
        "- 날씨가 맑고 기온이 25도 이상이면 수영을 하고 30도가 넘으면 영화를 본다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YR1Urg_-2lN"
      },
      "source": [
        "![결정트리.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZSU8vEh_GzE"
      },
      "source": [
        "## 2. 낙태 후 불임 가능성을 진단하는 classsification 모델을 작성하고 테스트 하시오."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbxNeLXRy6ww",
        "outputId": "b6efe8a5-cddb-4cdf-eed6-2596fed79658"
      },
      "source": [
        "# Load module\n",
        "!pip install pydataset\n",
        "from pydataset import data\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier    \n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn import metrics "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydataset\n",
            "  Downloading pydataset-0.2.0.tar.gz (15.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.9 MB 119 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from pydataset) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pydataset) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->pydataset) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas->pydataset) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->pydataset) (1.15.0)\n",
            "Building wheels for collected packages: pydataset\n",
            "  Building wheel for pydataset (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pydataset: filename=pydataset-0.2.0-py3-none-any.whl size=15939430 sha256=7dd245a78cc8bd329efa9086aa65d7581dcf1ec759e86f6fe6d06e93015895fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/32/26/30/d71562a19eed948eaada9a61b4d722fa358657a3bfb5d151e2\n",
            "Successfully built pydataset\n",
            "Installing collected packages: pydataset\n",
            "Successfully installed pydataset-0.2.0\n",
            "initiated datasets repo at: /root/.pydataset/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRsTtHGq4540",
        "outputId": "f9bc3efb-6469-4bd7-ebd1-80ba0b517633"
      },
      "source": [
        "# Load dataset\n",
        "data('infert', show_doc=True)  # 데이터 설명보기\n",
        "data = data('infert')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "infert\n",
            "\n",
            "PyDataset Documentation (adopted from R Documentation. The displayed examples are in R)\n",
            "\n",
            "## Infertility after Spontaneous and Induced Abortion\n",
            "\n",
            "### Description\n",
            "\n",
            "This is a matched case-control study dating from before the availability of\n",
            "conditional logistic regression.\n",
            "\n",
            "### Usage\n",
            "\n",
            "    infert\n",
            "\n",
            "### Format\n",
            "\n",
            "1\\.\n",
            "\n",
            "Education\n",
            "\n",
            "0 = 0-5 years\n",
            "\n",
            "1 = 6-11 years\n",
            "\n",
            "2 = 12+ years\n",
            "\n",
            "2\\.\n",
            "\n",
            "age\n",
            "\n",
            "age in years of case\n",
            "\n",
            "3\\.\n",
            "\n",
            "parity\n",
            "\n",
            "count\n",
            "\n",
            "4\\.\n",
            "\n",
            "number of prior\n",
            "\n",
            "0 = 0\n",
            "\n",
            "induced abortions\n",
            "\n",
            "1 = 1\n",
            "\n",
            "2 = 2 or more\n",
            "\n",
            "5\\.\n",
            "\n",
            "case status\n",
            "\n",
            "1 = case\n",
            "\n",
            "0 = control\n",
            "\n",
            "6\\.\n",
            "\n",
            "number of prior\n",
            "\n",
            "0 = 0\n",
            "\n",
            "spontaneous abortions\n",
            "\n",
            "1 = 1\n",
            "\n",
            "2 = 2 or more\n",
            "\n",
            "7\\.\n",
            "\n",
            "matched set number\n",
            "\n",
            "1-83\n",
            "\n",
            "8\\.\n",
            "\n",
            "stratum number\n",
            "\n",
            "1-63\n",
            "\n",
            "### Note\n",
            "\n",
            "One case with two prior spontaneous abortions and two prior induced abortions\n",
            "is omitted.\n",
            "\n",
            "### Source\n",
            "\n",
            "Trichopoulos _et al_ (1976) _Br. J. of Obst. and Gynaec._ **83**, 645–650.\n",
            "\n",
            "### Examples\n",
            "\n",
            "    require(stats)\n",
            "    model1 <- glm(case ~ spontaneous+induced, data = infert, family = binomial())\n",
            "    summary(model1)\n",
            "    ## adjusted for other potential confounders:\n",
            "    summary(model2 <- glm(case ~ age+parity+education+spontaneous+induced,\n",
            "                         data = infert, family = binomial()))\n",
            "    ## Really should be analysed by conditional logistic regression\n",
            "    ## which is in the survival package\n",
            "    if(require(survival)){\n",
            "      model3 <- clogit(case ~ spontaneous+induced+strata(stratum), data = infert)\n",
            "      print(summary(model3))\n",
            "      detach()  # survival (conflicts)\n",
            "    }\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2x5Uj7n5Bpm",
        "outputId": "18a7f14d-babb-4834-b63a-422b7e77c816"
      },
      "source": [
        "# Q1 : 데이터셋의 행과 열의 수를 보이시오\n",
        "data.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(248, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "T2me2t005XvI",
        "outputId": "bf68dd13-44c0-4ce6-9bba-1ac57304fc04"
      },
      "source": [
        "# Q2 : 데이터셋의 앞부분 10행의 데이터를 보이시오\n",
        "data.head(10)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>education</th>\n",
              "      <th>age</th>\n",
              "      <th>parity</th>\n",
              "      <th>induced</th>\n",
              "      <th>case</th>\n",
              "      <th>spontaneous</th>\n",
              "      <th>stratum</th>\n",
              "      <th>pooled.stratum</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0-5yrs</td>\n",
              "      <td>26</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0-5yrs</td>\n",
              "      <td>42</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0-5yrs</td>\n",
              "      <td>39</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0-5yrs</td>\n",
              "      <td>34</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6-11yrs</td>\n",
              "      <td>35</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6-11yrs</td>\n",
              "      <td>36</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>6-11yrs</td>\n",
              "      <td>23</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>6-11yrs</td>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>6-11yrs</td>\n",
              "      <td>21</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>6-11yrs</td>\n",
              "      <td>28</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   education  age  parity  induced  case  spontaneous  stratum  pooled.stratum\n",
              "1     0-5yrs   26       6        1     1            2        1               3\n",
              "2     0-5yrs   42       1        1     1            0        2               1\n",
              "3     0-5yrs   39       6        2     1            0        3               4\n",
              "4     0-5yrs   34       4        2     1            0        4               2\n",
              "5    6-11yrs   35       3        1     1            1        5              32\n",
              "6    6-11yrs   36       4        2     1            1        6              36\n",
              "7    6-11yrs   23       1        0     1            0        7               6\n",
              "8    6-11yrs   32       2        0     1            0        8              22\n",
              "9    6-11yrs   21       1        0     1            1        9               5\n",
              "10   6-11yrs   28       2        0     1            0       10              19"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdxQuvW_5Z-S",
        "outputId": "5675c5cb-6ac7-49fc-c426-b25657ef0565"
      },
      "source": [
        "# Q3 : 데이터셋의 컬럼(변수) 이름을 보이시오\n",
        "print(data.columns)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['education', 'age', 'parity', 'induced', 'case', 'spontaneous',\n",
            "       'stratum', 'pooled.stratum'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsPBjdN66Y-3"
      },
      "source": [
        "# Q4 : 데이터셋을 input variable 부분(X)과 target variable 부분(y)으로 나누시오 \n",
        "import pandas as pd\n",
        "data.education = data.education.factorize()[0].astype(float)\n",
        "\n",
        "X = data.loc[:, data.columns != 'case']\n",
        "y = data['case']"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KD_vFTdNwlvC",
        "outputId": "87b543c3-cc63-4b9e-da60-11978b8091f1"
      },
      "source": [
        "print(X.head())\n",
        "print(y.head())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   education  age  parity  induced  spontaneous  stratum  pooled.stratum\n",
            "1        0.0   26       6        1            2        1               3\n",
            "2        0.0   42       1        1            0        2               1\n",
            "3        0.0   39       6        2            0        3               4\n",
            "4        0.0   34       4        2            0        4               2\n",
            "5        1.0   35       3        1            1        5              32\n",
            "1    1\n",
            "2    1\n",
            "3    1\n",
            "4    1\n",
            "5    1\n",
            "Name: case, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iv1RJCTRxzB6"
      },
      "source": [
        "# Q5 : X, y 를  Train/Test set 으로 나누되 7:3 의 비율로 나누시오\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=1) "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYdjn27gy4KU",
        "outputId": "b7c9ce32-5cee-48ae-d701-c6c68dddb0dc"
      },
      "source": [
        "print(len(X_train))\n",
        "print(len(X_test))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "173\n",
            "75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5prb_hkM0pV7"
      },
      "source": [
        "# Q6 : DecisionTreeClassifier 로 진단 모델을 작성하시오 (Train set 이용)\n",
        "clf = DecisionTreeClassifier(random_state=100)\n",
        "model = clf.fit(X_train,y_train)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0N7IMYck5FuG",
        "outputId": "e7faa22f-f292-4be3-87f6-91e091aafa97"
      },
      "source": [
        "# Q7 : Test set으로 모델의 정확도(accuracy)를 평가하시오.\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6133333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Geidrl2mC0-O"
      },
      "source": [
        "## 3. 데이터셋에 대해 결정트리 모델을 만들고, 다음의 안내를 따라 feature selection 및 parameter tuning을 하시오. (dry_bean.csv)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgtthlJYuhgj"
      },
      "source": [
        "# Load module\n",
        "import pandas as pd \n",
        "from sklearn.tree import DecisionTreeClassifier    \n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn import metrics \n",
        "\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxD0bFN86tRS"
      },
      "source": [
        "# Load data\n",
        "data = pd.read_csv('/content/dry_bean.csv')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vf4lxjwcuZfO",
        "outputId": "9d7563a5-7074-415d-94b9-27fd27f41d6b"
      },
      "source": [
        "# Q1 : 데이터셋의 행과 열의 수를 보이시오\n",
        "data.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13611, 17)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "9EwfpiTsukZm",
        "outputId": "0985c210-1977-4247-a3d4-1be1d2f9e151"
      },
      "source": [
        "data.head(10)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Area</th>\n",
              "      <th>Perimeter</th>\n",
              "      <th>MajorAxisLength</th>\n",
              "      <th>MinorAxisLength</th>\n",
              "      <th>AspectRation</th>\n",
              "      <th>Eccentricity</th>\n",
              "      <th>ConvexArea</th>\n",
              "      <th>EquivDiameter</th>\n",
              "      <th>Extent</th>\n",
              "      <th>Solidity</th>\n",
              "      <th>roundness</th>\n",
              "      <th>Compactness</th>\n",
              "      <th>ShapeFactor1</th>\n",
              "      <th>ShapeFactor2</th>\n",
              "      <th>ShapeFactor3</th>\n",
              "      <th>ShapeFactor4</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>28395</td>\n",
              "      <td>610.291</td>\n",
              "      <td>208.178117</td>\n",
              "      <td>173.888747</td>\n",
              "      <td>1.197191</td>\n",
              "      <td>0.549812</td>\n",
              "      <td>28715</td>\n",
              "      <td>190.141097</td>\n",
              "      <td>0.763923</td>\n",
              "      <td>0.988856</td>\n",
              "      <td>0.958027</td>\n",
              "      <td>0.913358</td>\n",
              "      <td>0.007332</td>\n",
              "      <td>0.003147</td>\n",
              "      <td>0.834222</td>\n",
              "      <td>0.998724</td>\n",
              "      <td>SEKER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>28734</td>\n",
              "      <td>638.018</td>\n",
              "      <td>200.524796</td>\n",
              "      <td>182.734419</td>\n",
              "      <td>1.097356</td>\n",
              "      <td>0.411785</td>\n",
              "      <td>29172</td>\n",
              "      <td>191.272751</td>\n",
              "      <td>0.783968</td>\n",
              "      <td>0.984986</td>\n",
              "      <td>0.887034</td>\n",
              "      <td>0.953861</td>\n",
              "      <td>0.006979</td>\n",
              "      <td>0.003564</td>\n",
              "      <td>0.909851</td>\n",
              "      <td>0.998430</td>\n",
              "      <td>SEKER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>29380</td>\n",
              "      <td>624.110</td>\n",
              "      <td>212.826130</td>\n",
              "      <td>175.931143</td>\n",
              "      <td>1.209713</td>\n",
              "      <td>0.562727</td>\n",
              "      <td>29690</td>\n",
              "      <td>193.410904</td>\n",
              "      <td>0.778113</td>\n",
              "      <td>0.989559</td>\n",
              "      <td>0.947849</td>\n",
              "      <td>0.908774</td>\n",
              "      <td>0.007244</td>\n",
              "      <td>0.003048</td>\n",
              "      <td>0.825871</td>\n",
              "      <td>0.999066</td>\n",
              "      <td>SEKER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>30008</td>\n",
              "      <td>645.884</td>\n",
              "      <td>210.557999</td>\n",
              "      <td>182.516516</td>\n",
              "      <td>1.153638</td>\n",
              "      <td>0.498616</td>\n",
              "      <td>30724</td>\n",
              "      <td>195.467062</td>\n",
              "      <td>0.782681</td>\n",
              "      <td>0.976696</td>\n",
              "      <td>0.903936</td>\n",
              "      <td>0.928329</td>\n",
              "      <td>0.007017</td>\n",
              "      <td>0.003215</td>\n",
              "      <td>0.861794</td>\n",
              "      <td>0.994199</td>\n",
              "      <td>SEKER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>30140</td>\n",
              "      <td>620.134</td>\n",
              "      <td>201.847882</td>\n",
              "      <td>190.279279</td>\n",
              "      <td>1.060798</td>\n",
              "      <td>0.333680</td>\n",
              "      <td>30417</td>\n",
              "      <td>195.896503</td>\n",
              "      <td>0.773098</td>\n",
              "      <td>0.990893</td>\n",
              "      <td>0.984877</td>\n",
              "      <td>0.970516</td>\n",
              "      <td>0.006697</td>\n",
              "      <td>0.003665</td>\n",
              "      <td>0.941900</td>\n",
              "      <td>0.999166</td>\n",
              "      <td>SEKER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>30279</td>\n",
              "      <td>634.927</td>\n",
              "      <td>212.560556</td>\n",
              "      <td>181.510182</td>\n",
              "      <td>1.171067</td>\n",
              "      <td>0.520401</td>\n",
              "      <td>30600</td>\n",
              "      <td>196.347702</td>\n",
              "      <td>0.775688</td>\n",
              "      <td>0.989510</td>\n",
              "      <td>0.943852</td>\n",
              "      <td>0.923726</td>\n",
              "      <td>0.007020</td>\n",
              "      <td>0.003153</td>\n",
              "      <td>0.853270</td>\n",
              "      <td>0.999236</td>\n",
              "      <td>SEKER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>30477</td>\n",
              "      <td>670.033</td>\n",
              "      <td>211.050155</td>\n",
              "      <td>184.039050</td>\n",
              "      <td>1.146768</td>\n",
              "      <td>0.489478</td>\n",
              "      <td>30970</td>\n",
              "      <td>196.988633</td>\n",
              "      <td>0.762402</td>\n",
              "      <td>0.984081</td>\n",
              "      <td>0.853080</td>\n",
              "      <td>0.933374</td>\n",
              "      <td>0.006925</td>\n",
              "      <td>0.003242</td>\n",
              "      <td>0.871186</td>\n",
              "      <td>0.999049</td>\n",
              "      <td>SEKER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>30519</td>\n",
              "      <td>629.727</td>\n",
              "      <td>212.996755</td>\n",
              "      <td>182.737204</td>\n",
              "      <td>1.165591</td>\n",
              "      <td>0.513760</td>\n",
              "      <td>30847</td>\n",
              "      <td>197.124320</td>\n",
              "      <td>0.770682</td>\n",
              "      <td>0.989367</td>\n",
              "      <td>0.967109</td>\n",
              "      <td>0.925480</td>\n",
              "      <td>0.006979</td>\n",
              "      <td>0.003158</td>\n",
              "      <td>0.856514</td>\n",
              "      <td>0.998345</td>\n",
              "      <td>SEKER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>30685</td>\n",
              "      <td>635.681</td>\n",
              "      <td>213.534145</td>\n",
              "      <td>183.157146</td>\n",
              "      <td>1.165852</td>\n",
              "      <td>0.514081</td>\n",
              "      <td>31044</td>\n",
              "      <td>197.659696</td>\n",
              "      <td>0.771561</td>\n",
              "      <td>0.988436</td>\n",
              "      <td>0.954240</td>\n",
              "      <td>0.925658</td>\n",
              "      <td>0.006959</td>\n",
              "      <td>0.003152</td>\n",
              "      <td>0.856844</td>\n",
              "      <td>0.998953</td>\n",
              "      <td>SEKER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>30834</td>\n",
              "      <td>631.934</td>\n",
              "      <td>217.227813</td>\n",
              "      <td>180.897469</td>\n",
              "      <td>1.200834</td>\n",
              "      <td>0.553642</td>\n",
              "      <td>31120</td>\n",
              "      <td>198.139012</td>\n",
              "      <td>0.783683</td>\n",
              "      <td>0.990810</td>\n",
              "      <td>0.970278</td>\n",
              "      <td>0.912125</td>\n",
              "      <td>0.007045</td>\n",
              "      <td>0.003008</td>\n",
              "      <td>0.831973</td>\n",
              "      <td>0.999061</td>\n",
              "      <td>SEKER</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Area  Perimeter  MajorAxisLength  ...  ShapeFactor3  ShapeFactor4  Class\n",
              "0  28395    610.291       208.178117  ...      0.834222      0.998724  SEKER\n",
              "1  28734    638.018       200.524796  ...      0.909851      0.998430  SEKER\n",
              "2  29380    624.110       212.826130  ...      0.825871      0.999066  SEKER\n",
              "3  30008    645.884       210.557999  ...      0.861794      0.994199  SEKER\n",
              "4  30140    620.134       201.847882  ...      0.941900      0.999166  SEKER\n",
              "5  30279    634.927       212.560556  ...      0.853270      0.999236  SEKER\n",
              "6  30477    670.033       211.050155  ...      0.871186      0.999049  SEKER\n",
              "7  30519    629.727       212.996755  ...      0.856514      0.998345  SEKER\n",
              "8  30685    635.681       213.534145  ...      0.856844      0.998953  SEKER\n",
              "9  30834    631.934       217.227813  ...      0.831973      0.999061  SEKER\n",
              "\n",
              "[10 rows x 17 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZI0KVHIuloo",
        "outputId": "90bb9e06-f32b-474d-c88a-8354df2df3d8"
      },
      "source": [
        "print(data.columns)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Area', 'Perimeter', 'MajorAxisLength', 'MinorAxisLength',\n",
            "       'AspectRation', 'Eccentricity', 'ConvexArea', 'EquivDiameter', 'Extent',\n",
            "       'Solidity', 'roundness', 'Compactness', 'ShapeFactor1', 'ShapeFactor2',\n",
            "       'ShapeFactor3', 'ShapeFactor4', 'Class'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMY842oyunCw"
      },
      "source": [
        "# Q2 : 데이터셋을 input variable 부분(X)과 target variable 부분(y)으로 나누시오 \n",
        "X = data.iloc[:,data.columns != \"Class\"]\n",
        "y = data[\"Class\"]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U70VhpF8uot-"
      },
      "source": [
        "# Q3 : X, y 를  Train/Test set 으로 나누되 7:3 의 비율로 나누시오\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
        "                          test_size=0.3, random_state=1)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JhZuYVKDtdx",
        "outputId": "360d1d40-2a68-4c01-dd9a-70fb837adae6"
      },
      "source": [
        "# Q4 : 결정트리 모델(model_1)을 만들어 성능을 평가하시오.\n",
        "clf = DecisionTreeClassifier(random_state=2)\n",
        "model_1 = clf.fit(X_train,y_train)\n",
        "y_pred = model_1.predict(X_test)\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test,  y_pred))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8903036238981391\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hpvFP52uqo_",
        "outputId": "20bbb2a7-2127-445d-8792-9df118010af0"
      },
      "source": [
        "# Q5 : chi2를 이용하여 변수의 중요도를 평가하며, 15개의 변수를 선택하시오.\n",
        "chY = SelectKBest(chi2, k=15)\n",
        "X_new = chY.fit_transform(X, y)  # get new data\n",
        "print(X_new.shape)\n",
        "\n",
        "column_names = [column[0]  for column in zip(X.columns, chY.get_support()) if column[1]]\n",
        "print(column_names)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(13611, 15)\n",
            "['Area', 'Perimeter', 'MajorAxisLength', 'MinorAxisLength', 'AspectRation', 'Eccentricity', 'ConvexArea', 'EquivDiameter', 'Extent', 'roundness', 'Compactness', 'ShapeFactor1', 'ShapeFactor2', 'ShapeFactor3', 'ShapeFactor4']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jETss71W14VY",
        "outputId": "701fe56a-361f-463f-9445-c68d551a78de"
      },
      "source": [
        "# Q6 : Q5에서 선택된 15개의 변수들로 결정트리 모델(model_2)를 다시 세우고, Test 데이터로 성능을 평가하시오.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.3, random_state=1)\n",
        "\n",
        "clf = DecisionTreeClassifier(random_state=2)\n",
        "model_2 = clf.fit(X_train,y_train)\n",
        "y_pred2 = model_2.predict(X_test)\n",
        "\n",
        "print(\"Accuracy model2:\",metrics.accuracy_score(y_test, y_pred2))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy model2: 0.8929970617042116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5PwaxhgE8-Kp",
        "outputId": "c46924fd-29c9-4c9f-b7ae-ae8dd8805063"
      },
      "source": [
        "# Q7 : Q5 에서 선택된 변수로 구성된 데이터에 대해 gridsearchcv로 결정트리의 최적의 파라미터를 찾고, \n",
        "# 해당 파라미터로 결정트리(model_3)를 만들어 성능을 평가하시오.\n",
        "clf = DecisionTreeClassifier(random_state=2)\n",
        "\n",
        "parameters = {'max_depth': [3, 5, 10],\n",
        "              'min_samples_split': [4, 5, 6],\n",
        "              'splitter': ['best', 'random'],\n",
        "              'min_samples_leaf': [5,7,9]}\n",
        "\n",
        "grid_dt = GridSearchCV(clf, param_grid = parameters, cv = 5) # 교차검증 5개\n",
        "\n",
        "grid_dt.fit(X_new, y)\n",
        "\n",
        "result = pd.DataFrame(grid_dt.cv_results_['params'])\n",
        "result['mean_test_score'] = grid_dt.cv_results_['mean_test_score']\n",
        "result.sort_values(by='mean_test_score', ascending=False)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>max_depth</th>\n",
              "      <th>min_samples_leaf</th>\n",
              "      <th>min_samples_split</th>\n",
              "      <th>splitter</th>\n",
              "      <th>mean_test_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>random</td>\n",
              "      <td>0.748449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>random</td>\n",
              "      <td>0.748449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>random</td>\n",
              "      <td>0.748449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>random</td>\n",
              "      <td>0.737281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>random</td>\n",
              "      <td>0.737281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>random</td>\n",
              "      <td>0.737281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>random</td>\n",
              "      <td>0.725895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>random</td>\n",
              "      <td>0.725895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>random</td>\n",
              "      <td>0.725895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>10</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>random</td>\n",
              "      <td>0.712895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>10</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>random</td>\n",
              "      <td>0.712895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>10</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>random</td>\n",
              "      <td>0.712895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>random</td>\n",
              "      <td>0.710321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>random</td>\n",
              "      <td>0.710321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>random</td>\n",
              "      <td>0.710321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>best</td>\n",
              "      <td>0.682767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>best</td>\n",
              "      <td>0.682767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>best</td>\n",
              "      <td>0.682767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>best</td>\n",
              "      <td>0.682693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>best</td>\n",
              "      <td>0.682693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>best</td>\n",
              "      <td>0.682693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>random</td>\n",
              "      <td>0.671673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>random</td>\n",
              "      <td>0.671673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>random</td>\n",
              "      <td>0.671673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>random</td>\n",
              "      <td>0.655070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>random</td>\n",
              "      <td>0.655070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>random</td>\n",
              "      <td>0.655070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>random</td>\n",
              "      <td>0.655070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>random</td>\n",
              "      <td>0.655070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>random</td>\n",
              "      <td>0.655070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>random</td>\n",
              "      <td>0.655070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>random</td>\n",
              "      <td>0.655070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>random</td>\n",
              "      <td>0.655070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>10</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>best</td>\n",
              "      <td>0.651689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>10</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>best</td>\n",
              "      <td>0.651689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>10</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>best</td>\n",
              "      <td>0.651689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>best</td>\n",
              "      <td>0.643974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>best</td>\n",
              "      <td>0.643974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>best</td>\n",
              "      <td>0.643974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>best</td>\n",
              "      <td>0.623840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>best</td>\n",
              "      <td>0.623840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>best</td>\n",
              "      <td>0.623840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>best</td>\n",
              "      <td>0.594672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>best</td>\n",
              "      <td>0.594672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>best</td>\n",
              "      <td>0.594672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>best</td>\n",
              "      <td>0.593131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>best</td>\n",
              "      <td>0.593131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>best</td>\n",
              "      <td>0.593131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>best</td>\n",
              "      <td>0.593131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>best</td>\n",
              "      <td>0.593131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>best</td>\n",
              "      <td>0.593131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>best</td>\n",
              "      <td>0.593131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>best</td>\n",
              "      <td>0.593131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>best</td>\n",
              "      <td>0.593131</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    max_depth  min_samples_leaf  min_samples_split splitter  mean_test_score\n",
              "27          5                 7                  5   random         0.748449\n",
              "29          5                 7                  6   random         0.748449\n",
              "25          5                 7                  4   random         0.748449\n",
              "19          5                 5                  4   random         0.737281\n",
              "23          5                 5                  6   random         0.737281\n",
              "21          5                 5                  5   random         0.737281\n",
              "35          5                 9                  6   random         0.725895\n",
              "33          5                 9                  5   random         0.725895\n",
              "31          5                 9                  4   random         0.725895\n",
              "51         10                 9                  5   random         0.712895\n",
              "49         10                 9                  4   random         0.712895\n",
              "53         10                 9                  6   random         0.712895\n",
              "45         10                 7                  5   random         0.710321\n",
              "43         10                 7                  4   random         0.710321\n",
              "47         10                 7                  6   random         0.710321\n",
              "34          5                 9                  6     best         0.682767\n",
              "32          5                 9                  5     best         0.682767\n",
              "30          5                 9                  4     best         0.682767\n",
              "26          5                 7                  5     best         0.682693\n",
              "28          5                 7                  6     best         0.682693\n",
              "24          5                 7                  4     best         0.682693\n",
              "41         10                 5                  6   random         0.671673\n",
              "39         10                 5                  5   random         0.671673\n",
              "37         10                 5                  4   random         0.671673\n",
              "13          3                 9                  4   random         0.655070\n",
              "1           3                 5                  4   random         0.655070\n",
              "5           3                 5                  6   random         0.655070\n",
              "3           3                 5                  5   random         0.655070\n",
              "11          3                 7                  6   random         0.655070\n",
              "15          3                 9                  5   random         0.655070\n",
              "17          3                 9                  6   random         0.655070\n",
              "9           3                 7                  5   random         0.655070\n",
              "7           3                 7                  4   random         0.655070\n",
              "50         10                 9                  5     best         0.651689\n",
              "48         10                 9                  4     best         0.651689\n",
              "52         10                 9                  6     best         0.651689\n",
              "46         10                 7                  6     best         0.643974\n",
              "42         10                 7                  4     best         0.643974\n",
              "44         10                 7                  5     best         0.643974\n",
              "20          5                 5                  5     best         0.623840\n",
              "18          5                 5                  4     best         0.623840\n",
              "22          5                 5                  6     best         0.623840\n",
              "38         10                 5                  5     best         0.594672\n",
              "40         10                 5                  6     best         0.594672\n",
              "36         10                 5                  4     best         0.594672\n",
              "2           3                 5                  5     best         0.593131\n",
              "12          3                 9                  4     best         0.593131\n",
              "4           3                 5                  6     best         0.593131\n",
              "6           3                 7                  4     best         0.593131\n",
              "8           3                 7                  5     best         0.593131\n",
              "10          3                 7                  6     best         0.593131\n",
              "16          3                 9                  6     best         0.593131\n",
              "14          3                 9                  5     best         0.593131\n",
              "0           3                 5                  4     best         0.593131"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vZ9-Gzk-cgK",
        "outputId": "f0156b8f-a7a0-48b1-d27a-d113d21c9524"
      },
      "source": [
        "# Q7 : 성능평가\n",
        "clf = DecisionTreeClassifier(random_state=2, \n",
        "           max_depth=5, min_samples_split=5, min_samples_leaf=7,splitter=\"random\")\n",
        "model_3 = clf.fit(X_train,y_train)\n",
        "y_pred3 = model_3.predict(X_test)\n",
        "\n",
        "print(\"Accuracy model3:\",metrics.accuracy_score(y_test, y_pred3))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy model3: 0.8731635651322233\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXT0kMWK-2EX",
        "outputId": "e681cb28-f284-4db3-dba5-6f4887e35bb5"
      },
      "source": [
        "y_test.value_counts()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DERMASON    1062\n",
              "SIRA         800\n",
              "SEKER        621\n",
              "HOROZ        579\n",
              "CALI         479\n",
              "BARBUNYA     390\n",
              "BOMBAY       153\n",
              "Name: Class, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N9prjG1uJ74B",
        "outputId": "a4153d6c-bf5f-41fb-9e86-fab791563b33"
      },
      "source": [
        "result['std_test_score'] = grid_dt.cv_results_['std_test_score']\n",
        "result.sort_values(by='std_test_score', ascending=False)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>max_depth</th>\n",
              "      <th>min_samples_leaf</th>\n",
              "      <th>min_samples_split</th>\n",
              "      <th>splitter</th>\n",
              "      <th>mean_test_score</th>\n",
              "      <th>std_test_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>best</td>\n",
              "      <td>0.623840</td>\n",
              "      <td>0.224711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>best</td>\n",
              "      <td>0.623840</td>\n",
              "      <td>0.224711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>best</td>\n",
              "      <td>0.623840</td>\n",
              "      <td>0.224711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>best</td>\n",
              "      <td>0.682767</td>\n",
              "      <td>0.222621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>best</td>\n",
              "      <td>0.682767</td>\n",
              "      <td>0.222621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>best</td>\n",
              "      <td>0.682767</td>\n",
              "      <td>0.222621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>best</td>\n",
              "      <td>0.682693</td>\n",
              "      <td>0.222597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>best</td>\n",
              "      <td>0.682693</td>\n",
              "      <td>0.222597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>best</td>\n",
              "      <td>0.682693</td>\n",
              "      <td>0.222597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>10</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>best</td>\n",
              "      <td>0.651689</td>\n",
              "      <td>0.208222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>10</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>best</td>\n",
              "      <td>0.651689</td>\n",
              "      <td>0.208222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>10</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>best</td>\n",
              "      <td>0.651689</td>\n",
              "      <td>0.208222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>best</td>\n",
              "      <td>0.643974</td>\n",
              "      <td>0.203792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>best</td>\n",
              "      <td>0.643974</td>\n",
              "      <td>0.203792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>best</td>\n",
              "      <td>0.643974</td>\n",
              "      <td>0.203792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>random</td>\n",
              "      <td>0.725895</td>\n",
              "      <td>0.199867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>random</td>\n",
              "      <td>0.725895</td>\n",
              "      <td>0.199867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>random</td>\n",
              "      <td>0.725895</td>\n",
              "      <td>0.199867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>random</td>\n",
              "      <td>0.737281</td>\n",
              "      <td>0.198034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>random</td>\n",
              "      <td>0.737281</td>\n",
              "      <td>0.198034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>random</td>\n",
              "      <td>0.737281</td>\n",
              "      <td>0.198034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>best</td>\n",
              "      <td>0.594672</td>\n",
              "      <td>0.184825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>best</td>\n",
              "      <td>0.594672</td>\n",
              "      <td>0.184825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>best</td>\n",
              "      <td>0.594672</td>\n",
              "      <td>0.184825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>random</td>\n",
              "      <td>0.710321</td>\n",
              "      <td>0.182348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>random</td>\n",
              "      <td>0.710321</td>\n",
              "      <td>0.182348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>random</td>\n",
              "      <td>0.710321</td>\n",
              "      <td>0.182348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>random</td>\n",
              "      <td>0.671673</td>\n",
              "      <td>0.181836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>random</td>\n",
              "      <td>0.671673</td>\n",
              "      <td>0.181836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>random</td>\n",
              "      <td>0.671673</td>\n",
              "      <td>0.181836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>10</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>random</td>\n",
              "      <td>0.712895</td>\n",
              "      <td>0.181189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>10</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>random</td>\n",
              "      <td>0.712895</td>\n",
              "      <td>0.181189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>10</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>random</td>\n",
              "      <td>0.712895</td>\n",
              "      <td>0.181189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>random</td>\n",
              "      <td>0.748449</td>\n",
              "      <td>0.180260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>random</td>\n",
              "      <td>0.748449</td>\n",
              "      <td>0.180260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>random</td>\n",
              "      <td>0.748449</td>\n",
              "      <td>0.180260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>random</td>\n",
              "      <td>0.655070</td>\n",
              "      <td>0.180184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>random</td>\n",
              "      <td>0.655070</td>\n",
              "      <td>0.180184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>random</td>\n",
              "      <td>0.655070</td>\n",
              "      <td>0.180184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>random</td>\n",
              "      <td>0.655070</td>\n",
              "      <td>0.180184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>random</td>\n",
              "      <td>0.655070</td>\n",
              "      <td>0.180184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>random</td>\n",
              "      <td>0.655070</td>\n",
              "      <td>0.180184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>random</td>\n",
              "      <td>0.655070</td>\n",
              "      <td>0.180184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>random</td>\n",
              "      <td>0.655070</td>\n",
              "      <td>0.180184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>random</td>\n",
              "      <td>0.655070</td>\n",
              "      <td>0.180184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>best</td>\n",
              "      <td>0.593131</td>\n",
              "      <td>0.170504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>best</td>\n",
              "      <td>0.593131</td>\n",
              "      <td>0.170504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>best</td>\n",
              "      <td>0.593131</td>\n",
              "      <td>0.170504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>best</td>\n",
              "      <td>0.593131</td>\n",
              "      <td>0.170504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>best</td>\n",
              "      <td>0.593131</td>\n",
              "      <td>0.170504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>best</td>\n",
              "      <td>0.593131</td>\n",
              "      <td>0.170504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>best</td>\n",
              "      <td>0.593131</td>\n",
              "      <td>0.170504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>best</td>\n",
              "      <td>0.593131</td>\n",
              "      <td>0.170504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>best</td>\n",
              "      <td>0.593131</td>\n",
              "      <td>0.170504</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    max_depth  min_samples_leaf  ...  mean_test_score std_test_score\n",
              "20          5                 5  ...         0.623840       0.224711\n",
              "22          5                 5  ...         0.623840       0.224711\n",
              "18          5                 5  ...         0.623840       0.224711\n",
              "32          5                 9  ...         0.682767       0.222621\n",
              "30          5                 9  ...         0.682767       0.222621\n",
              "34          5                 9  ...         0.682767       0.222621\n",
              "28          5                 7  ...         0.682693       0.222597\n",
              "26          5                 7  ...         0.682693       0.222597\n",
              "24          5                 7  ...         0.682693       0.222597\n",
              "52         10                 9  ...         0.651689       0.208222\n",
              "50         10                 9  ...         0.651689       0.208222\n",
              "48         10                 9  ...         0.651689       0.208222\n",
              "46         10                 7  ...         0.643974       0.203792\n",
              "44         10                 7  ...         0.643974       0.203792\n",
              "42         10                 7  ...         0.643974       0.203792\n",
              "33          5                 9  ...         0.725895       0.199867\n",
              "35          5                 9  ...         0.725895       0.199867\n",
              "31          5                 9  ...         0.725895       0.199867\n",
              "23          5                 5  ...         0.737281       0.198034\n",
              "19          5                 5  ...         0.737281       0.198034\n",
              "21          5                 5  ...         0.737281       0.198034\n",
              "36         10                 5  ...         0.594672       0.184825\n",
              "38         10                 5  ...         0.594672       0.184825\n",
              "40         10                 5  ...         0.594672       0.184825\n",
              "43         10                 7  ...         0.710321       0.182348\n",
              "45         10                 7  ...         0.710321       0.182348\n",
              "47         10                 7  ...         0.710321       0.182348\n",
              "37         10                 5  ...         0.671673       0.181836\n",
              "39         10                 5  ...         0.671673       0.181836\n",
              "41         10                 5  ...         0.671673       0.181836\n",
              "49         10                 9  ...         0.712895       0.181189\n",
              "51         10                 9  ...         0.712895       0.181189\n",
              "53         10                 9  ...         0.712895       0.181189\n",
              "27          5                 7  ...         0.748449       0.180260\n",
              "29          5                 7  ...         0.748449       0.180260\n",
              "25          5                 7  ...         0.748449       0.180260\n",
              "17          3                 9  ...         0.655070       0.180184\n",
              "15          3                 9  ...         0.655070       0.180184\n",
              "3           3                 5  ...         0.655070       0.180184\n",
              "13          3                 9  ...         0.655070       0.180184\n",
              "11          3                 7  ...         0.655070       0.180184\n",
              "1           3                 5  ...         0.655070       0.180184\n",
              "9           3                 7  ...         0.655070       0.180184\n",
              "5           3                 5  ...         0.655070       0.180184\n",
              "7           3                 7  ...         0.655070       0.180184\n",
              "6           3                 7  ...         0.593131       0.170504\n",
              "2           3                 5  ...         0.593131       0.170504\n",
              "4           3                 5  ...         0.593131       0.170504\n",
              "10          3                 7  ...         0.593131       0.170504\n",
              "8           3                 7  ...         0.593131       0.170504\n",
              "12          3                 9  ...         0.593131       0.170504\n",
              "14          3                 9  ...         0.593131       0.170504\n",
              "16          3                 9  ...         0.593131       0.170504\n",
              "0           3                 5  ...         0.593131       0.170504\n",
              "\n",
              "[54 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16I8OJ7ZSTZ1"
      },
      "source": [
        "## 4. unit31_1.py 를 참조하여, 제공된 데이터셋에 대해 simple voting 방법으로 다음 안내에 따라 모델을 만들고 성능을 평가하시오."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-xJdEFDVPPY"
      },
      "source": [
        "# Load module \n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coSGMCoWVOXJ"
      },
      "source": [
        "# Load data\n",
        "data = pd.read_csv('/content/dry_bean.csv')"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNVwgM67Svsu"
      },
      "source": [
        "# Q1 : 데이터셋을 input variable 부분(X)과 target variable 부분(y)으로 나누시오. \n",
        "X = data.iloc[:,data.columns != \"Class\"]\n",
        "y = data[\"Class\"]"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO5VyghPSyBF"
      },
      "source": [
        "# Q2 : X, y 를 Train/Test set 으로 나누되 7:3 의 비율로 나누시오. (random_state = 1) \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) "
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuaIDHhNVRDM",
        "outputId": "42cc4043-55ff-4ade-9e5b-a72d9cbdddac"
      },
      "source": [
        "# Q3 : LogisticRegression, KNeighborsClassifier, DecisionTreeClassifier 3개의 모델을 각각 만들고 \n",
        "# 각 모델들의 성능을 평가하시오. (random_state=2, n_neighbors=1)\n",
        "clf_lr = LogisticRegression()\n",
        "clf_knn = KNeighborsClassifier(n_neighbors=1)\n",
        "clf_dt = DecisionTreeClassifier(random_state=2)\n",
        "\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test  = scaler.transform(X_test)\n",
        "\n",
        "models = [clf_lr, clf_knn, clf_dt]\n",
        "for model in models:\n",
        "    model.fit(X_train, y_train)\n",
        "    pred_y = model.predict(X_test)\n",
        "    model_name = model.__class__.__name__\n",
        "    print(f\"{model_name} \\t : {accuracy_score(y_test, pred_y)}\") \n"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression \t : 0.9236043095004897\n",
            "KNeighborsClassifier \t : 0.9037708129285015\n",
            "DecisionTreeClassifier \t : 0.8903036238981391\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vm1CAGexVq2j",
        "outputId": "1925cc5e-b08f-45cb-8e85-acfbd144f163"
      },
      "source": [
        "# Q4 : Q3에서의 3개의 모델을 앙상블하여 soft voting 방식으로 test 데이터를 예측하여 성능을 평가하시오.\n",
        "# Define voting classifer\n",
        "clf_voting = VotingClassifier(estimators=[('LR', clf_lr),\n",
        "                                        ('KNN', clf_knn),\n",
        "                                        ('DT', clf_dt)],voting='soft')\n",
        "clf_voting.fit(X_train, y_train)\n",
        "pred_y = clf_voting.predict(X_test)\n",
        "print('Voting accuracy', accuracy_score(y_test, pred_y))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Voting accuracy 0.92384916748286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfEBE4CCT2Td"
      },
      "source": [
        "# 실습 2\n",
        "## 1. 제공된 데이터셋에 대해 분류(classification) 모델을 만들고 성능을 평가 하시오."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzMep3SHV63x"
      },
      "source": [
        "# Load module\n",
        "from pprint import pprint\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn import metrics \n",
        "\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUV1HX1Rl92z",
        "outputId": "6ad6e4f4-56d7-498b-94fd-4a3657f341e5"
      },
      "source": [
        "# Load data\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "print(X.shape)       # (569, 30)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(569, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nf1BpeCyl_xN"
      },
      "source": [
        "# Q1 : X, y 를 Train/Test set 으로 나누되 7:3 의 비율로 나누시오. (random_state = 1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kF_RZCJMmCMX",
        "outputId": "c0fed0c0-7e02-4fad-c62a-7070554d7950"
      },
      "source": [
        "# Q2 : KNN, SVM, Random Forest 모델을 각각 구축하고, 모델 성능을 평가하시오. (필요시 표준화) \n",
        "clf_knn = KNeighborsClassifier()\n",
        "clf_svm = svm.SVC()\n",
        "clf_rf = RandomForestClassifier(random_state=100)\n",
        "\n",
        "# standarize \n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "\n",
        "X_train_sc = scaler.transform(X_train)\n",
        "X_test_sc = scaler.transform(X_test)\n",
        "\n",
        "# test KNN\n",
        "model = clf_knn.fit(X_train_sc,y_train)\n",
        "y_pred = model.predict(X_test_sc)\n",
        "print(\"KNN Accuracy:\",metrics.accuracy_score(y_test, y_pred))  # 0.96\n",
        "\n",
        "# test SVM\n",
        "model = clf_svm.fit(X_train_sc,y_train)\n",
        "y_pred = model.predict(X_test_sc)\n",
        "print(\"SVM Accuracy:\",metrics.accuracy_score(y_test, y_pred))  # 0.97\n",
        "\n",
        "# test Random Forest\n",
        "model = clf_rf.fit(X_train,y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"RF Accuracy:\",metrics.accuracy_score(y_test, y_pred))   # 0.9590"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Accuracy: 0.9590643274853801\n",
            "SVM Accuracy: 0.9707602339181286\n",
            "RF Accuracy: 0.9590643274853801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJtZbQGVXbt5",
        "outputId": "b9de576b-233b-4470-b3d1-d800b3e2ce81"
      },
      "source": [
        "# Q3 : 각 모델들에 대해 chi2를 이용하여 적절한 변수를 선택하고 각 모델의 성능을 평가하시오.\n",
        "\n",
        "# knn\n",
        "for i in range(2,X.shape[1]+1):\n",
        "    X_new = SelectKBest(chi2, k=i).fit_transform(X,y) # i개의 변수 선택\n",
        "    X_new_sc = StandardScaler().fit_transform(X_new) # 선택된 변수 데이터 표준화\n",
        "    scores = cross_val_score(clf_knn, X_new_sc, y, cv=5)\n",
        "    print(i, \"Mean Accuracy:\", scores.mean()) #25"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 Mean Accuracy: 0.924406148113647\n",
            "3 Mean Accuracy: 0.9262071106970968\n",
            "4 Mean Accuracy: 0.9367955286446203\n",
            "5 Mean Accuracy: 0.9350256171401956\n",
            "6 Mean Accuracy: 0.9280080732805466\n",
            "7 Mean Accuracy: 0.9315168452103709\n",
            "8 Mean Accuracy: 0.9209905294208973\n",
            "9 Mean Accuracy: 0.9262381617761216\n",
            "10 Mean Accuracy: 0.929762459245459\n",
            "11 Mean Accuracy: 0.9455364073901567\n",
            "12 Mean Accuracy: 0.9473218444340941\n",
            "13 Mean Accuracy: 0.947290793355069\n",
            "14 Mean Accuracy: 0.9543393882937432\n",
            "15 Mean Accuracy: 0.9578326346840551\n",
            "16 Mean Accuracy: 0.9578326346840551\n",
            "17 Mean Accuracy: 0.9560627231796305\n",
            "18 Mean Accuracy: 0.9543238627542306\n",
            "19 Mean Accuracy: 0.9578326346840551\n",
            "20 Mean Accuracy: 0.9648657040832168\n",
            "21 Mean Accuracy: 0.968390001552554\n",
            "22 Mean Accuracy: 0.9718832479428661\n",
            "23 Mean Accuracy: 0.968390001552554\n",
            "24 Mean Accuracy: 0.9701288619779538\n",
            "25 Mean Accuracy: 0.9754075454122031\n",
            "26 Mean Accuracy: 0.9701288619779538\n",
            "27 Mean Accuracy: 0.971883247942866\n",
            "28 Mean Accuracy: 0.9683744760130415\n",
            "29 Mean Accuracy: 0.9701288619779538\n",
            "30 Mean Accuracy: 0.9648501785437045\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClVc2OdD2Sgq",
        "outputId": "415097f2-62e1-4fcd-e132-e5b57458b54e"
      },
      "source": [
        "# Find selected feature name : knn\n",
        "chY_knn = SelectKBest(chi2, k=25)    # please test other k value\n",
        "X_new_knn = chY_knn.fit_transform(X, y)  # get new data (feature selected) \n",
        "column_id = [column[0]  for column in zip(range(0,30), chY_knn.get_support()) if column[1]]\n",
        "print(column_id)"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 12, 13, 15, 16, 17, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhgY_bLuWoSe",
        "outputId": "81202afd-36ae-488f-82b4-b4ce8666a114"
      },
      "source": [
        "# svm\n",
        "for i in range(2,X.shape[1]+1):\n",
        "    X_new = SelectKBest(chi2, k=i).fit_transform(X,y) # i개의 변수 선택\n",
        "    X_new_sc = StandardScaler().fit_transform(X_new) # 선택된 변수 데이터 표준화\n",
        "    scores = cross_val_score(clf_svm, X_new_sc, y, cv=5)\n",
        "    print(i, \"Mean Accuracy:\", scores.mean()) #23"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 Mean Accuracy: 0.9227293898462972\n",
            "3 Mean Accuracy: 0.9262071106970968\n",
            "4 Mean Accuracy: 0.9332712311752832\n",
            "5 Mean Accuracy: 0.9315168452103709\n",
            "6 Mean Accuracy: 0.9262536873156341\n",
            "7 Mean Accuracy: 0.9297624592454585\n",
            "8 Mean Accuracy: 0.9315168452103709\n",
            "9 Mean Accuracy: 0.9455519329296692\n",
            "10 Mean Accuracy: 0.9455519329296692\n",
            "11 Mean Accuracy: 0.95960254618848\n",
            "12 Mean Accuracy: 0.9560937742586555\n",
            "13 Mean Accuracy: 0.9543549138332557\n",
            "14 Mean Accuracy: 0.9578636857630801\n",
            "15 Mean Accuracy: 0.9666356155876417\n",
            "16 Mean Accuracy: 0.9683900015525537\n",
            "17 Mean Accuracy: 0.9701443875174661\n",
            "18 Mean Accuracy: 0.9701288619779538\n",
            "19 Mean Accuracy: 0.9718987734823784\n",
            "20 Mean Accuracy: 0.9736531594472908\n",
            "21 Mean Accuracy: 0.9754075454122031\n",
            "22 Mean Accuracy: 0.9754075454122031\n",
            "23 Mean Accuracy: 0.9771774569166277\n",
            "24 Mean Accuracy: 0.9736531594472908\n",
            "25 Mean Accuracy: 0.9754230709517155\n",
            "26 Mean Accuracy: 0.9683744760130415\n",
            "27 Mean Accuracy: 0.9718987734823784\n",
            "28 Mean Accuracy: 0.9754075454122031\n",
            "29 Mean Accuracy: 0.9753920198726906\n",
            "30 Mean Accuracy: 0.9736376339077782\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLi5mqJHYX6w",
        "outputId": "4b2f6d36-66e4-49ca-9ff5-aa257386a638"
      },
      "source": [
        "# Find selected feature name : svm\n",
        "chY_svm = SelectKBest(chi2, k=23)    # please test other k value\n",
        "X_new_svm = chY_svm.fit_transform(X, y)  # get new data (feature selected) \n",
        "column_id = [column[0]  for column in zip(range(0,30), chY_svm.get_support()) if column[1]]\n",
        "print(column_id)"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 5, 6, 7, 8, 10, 12, 13, 15, 16, 17, 20, 21, 22, 23, 24, 25, 26, 27, 28]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Jzt8u5PzWwp",
        "outputId": "816e2450-fc23-413a-900f-3f94c1535927"
      },
      "source": [
        "# random forest\n",
        "for i in range(2,X.shape[1]+1):\n",
        "    X_new = SelectKBest(chi2, k=i).fit_transform(X,y)\n",
        "    scores = cross_val_score(clf_rf, X_new, y, cv=5)\n",
        "    print(i, \"Mean Accuracy:\", scores.mean()) # 17"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 Mean Accuracy: 0.9174041297935103\n",
            "3 Mean Accuracy: 0.9156963204471354\n",
            "4 Mean Accuracy: 0.9420586865393572\n",
            "5 Mean Accuracy: 0.9315168452103709\n",
            "6 Mean Accuracy: 0.9385499146095327\n",
            "7 Mean Accuracy: 0.9297469337059463\n",
            "8 Mean Accuracy: 0.9280080732805466\n",
            "9 Mean Accuracy: 0.9367489520260829\n",
            "10 Mean Accuracy: 0.9420276354603322\n",
            "11 Mean Accuracy: 0.9490762303990063\n",
            "12 Mean Accuracy: 0.9490762303990063\n",
            "13 Mean Accuracy: 0.9490762303990063\n",
            "14 Mean Accuracy: 0.9561092997981679\n",
            "15 Mean Accuracy: 0.9525694767893185\n",
            "16 Mean Accuracy: 0.9613724576929048\n",
            "17 Mean Accuracy: 0.9666511411271541\n",
            "18 Mean Accuracy: 0.9596180717279925\n",
            "19 Mean Accuracy: 0.9631268436578171\n",
            "20 Mean Accuracy: 0.9631423691973297\n",
            "21 Mean Accuracy: 0.9613724576929048\n",
            "22 Mean Accuracy: 0.9578481602235677\n",
            "23 Mean Accuracy: 0.9543393882937432\n",
            "24 Mean Accuracy: 0.9596180717279925\n",
            "25 Mean Accuracy: 0.9648657040832168\n",
            "26 Mean Accuracy: 0.9631268436578171\n",
            "27 Mean Accuracy: 0.9666356155876418\n",
            "28 Mean Accuracy: 0.9578636857630801\n",
            "29 Mean Accuracy: 0.9666511411271541\n",
            "30 Mean Accuracy: 0.9613569321533924\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6CieQ_6z1WX",
        "outputId": "cec2b3dc-ba79-4644-b2d8-841187e0e9c5"
      },
      "source": [
        "# Find selected feature name : rf\n",
        "chY = SelectKBest(chi2, k=17)    # please test other k value \n",
        "X_new = chY.fit_transform(X, y)  # get new data (feature selected) \n",
        "column_id = [column[0]  for column in zip(range(0,30), chY.get_support()) if column[1]]\n",
        "print(column_id)"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 5, 6, 7, 10, 12, 13, 20, 21, 22, 23, 25, 26, 27]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLBa8Kjq0oBj"
      },
      "source": [
        "# train_test split\n",
        "X_train_knn, X_test_knn, y_train_knn, y_test_knn = train_test_split(X_new_knn, y, test_size=0.3, random_state=1)\n",
        "X_train_svm, X_test_svm, y_train_svm, y_test_svm = train_test_split(X_new_svm, y, test_size=0.3, random_state=1)\n",
        "X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X_new, y, test_size=0.3, random_state=1)"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RRH5ksh2cd4",
        "outputId": "4043445d-53a5-47fc-d5ae-54814fb98fe8"
      },
      "source": [
        "print(X_train_knn.shape)\n",
        "print(X_train_svm.shape)\n",
        "print(X_train_rf.shape)"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(398, 25)\n",
            "(398, 23)\n",
            "(398, 17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsugOMY-0uAU",
        "outputId": "78b8b629-3efe-4061-c5e7-dcf07072b14b"
      },
      "source": [
        "# predict\n",
        "scaler_knn = StandardScaler().fit(X_train_knn)\n",
        "scaler_svm = StandardScaler().fit(X_train_svm)\n",
        "\n",
        "X_train_knn = scaler_knn.transform(X_train_knn)\n",
        "X_test_knn = scaler_knn.transform(X_test_knn)\n",
        "X_train_svm = scaler_svm.transform(X_train_svm)\n",
        "X_test_svm = scaler_svm.transform(X_test_svm)\n",
        "\n",
        "# test KNN\n",
        "model = clf_knn.fit(X_train_knn,y_train)\n",
        "y_pred_knn = model.predict(X_test_knn)\n",
        "print(\"KNN Accuracy:\",metrics.accuracy_score(y_test, y_pred_knn))  # 0.9532\n",
        "\n",
        "# test SVM\n",
        "model = clf_svm.fit(X_train_svm,y_train)\n",
        "y_pred_svm = model.predict(X_test_svm)\n",
        "print(\"SVM Accuracy:\",metrics.accuracy_score(y_test, y_pred_svm))  # 0.9707\n",
        "\n",
        "# test Random Forest\n",
        "model = clf_rf.fit(X_train_rf,y_train)\n",
        "y_pred_rf = model.predict(X_test_rf)\n",
        "print(\"RF Accuracy:\",metrics.accuracy_score(y_test, y_pred_rf))   # 0.9649"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Accuracy: 0.9532163742690059\n",
            "SVM Accuracy: 0.9707602339181286\n",
            "RF Accuracy: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjCtptDf2iTu",
        "outputId": "ecc49e9c-509e-4d67-fb64-63db19b7d071"
      },
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "\n",
        "X_train_sc = scaler.transform(X_train)\n",
        "X_test_sc = scaler.transform(X_test)\n",
        "\n",
        "# test KNN\n",
        "model = clf_knn.fit(X_train_sc,y_train)\n",
        "y_pred = model.predict(X_test_sc)\n",
        "print(\"KNN Accuracy:\",metrics.accuracy_score(y_test, y_pred))  # 0.9298"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Accuracy: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymOzP-bs1C_0",
        "outputId": "490f3a06-9b71-46ea-cdab-4ba1c38db4c2"
      },
      "source": [
        "# Q4 : 각 모델들의 최적의 파라미터를 찾고, 각 모델의 성능을 평가하시오.\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# KNN \n",
        "n_neighbors = [1, 3, 5, 7]\n",
        "weights = ['uniform', 'distance']\n",
        "\n",
        "dic_grid = {'n_neighbors': n_neighbors,\n",
        "            'weights': weights}\n",
        "pprint(dic_grid)\n",
        "\n",
        "# Parameter tuning (takes long time)\n",
        "knn_grid = GridSearchCV(estimator = clf_knn, param_grid = dic_grid, \n",
        "                                cv = 3, verbose=2)\n",
        "# Fit the grid search model\n",
        "X_new_knn_sc = StandardScaler().fit_transform(X_new_knn)\n",
        "knn_grid.fit(X_new_knn_sc, y)\n",
        "pprint(knn_grid.best_params_)\n"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_neighbors': [1, 3, 5, 7], 'weights': ['uniform', 'distance']}\n",
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
            "[CV] n_neighbors=1, weights=uniform ..................................\n",
            "[CV] ................... n_neighbors=1, weights=uniform, total=   0.0s\n",
            "[CV] n_neighbors=1, weights=uniform ..................................\n",
            "[CV] ................... n_neighbors=1, weights=uniform, total=   0.0s\n",
            "[CV] n_neighbors=1, weights=uniform ..................................\n",
            "[CV] ................... n_neighbors=1, weights=uniform, total=   0.0s\n",
            "[CV] n_neighbors=1, weights=distance .................................\n",
            "[CV] .................. n_neighbors=1, weights=distance, total=   0.0s\n",
            "[CV] n_neighbors=1, weights=distance .................................\n",
            "[CV] .................. n_neighbors=1, weights=distance, total=   0.0s\n",
            "[CV] n_neighbors=1, weights=distance .................................\n",
            "[CV] .................. n_neighbors=1, weights=distance, total=   0.0s\n",
            "[CV] n_neighbors=3, weights=uniform ..................................\n",
            "[CV] ................... n_neighbors=3, weights=uniform, total=   0.0s\n",
            "[CV] n_neighbors=3, weights=uniform ..................................\n",
            "[CV] ................... n_neighbors=3, weights=uniform, total=   0.0s\n",
            "[CV] n_neighbors=3, weights=uniform ..................................\n",
            "[CV] ................... n_neighbors=3, weights=uniform, total=   0.0s\n",
            "[CV] n_neighbors=3, weights=distance .................................\n",
            "[CV] .................. n_neighbors=3, weights=distance, total=   0.0s\n",
            "[CV] n_neighbors=3, weights=distance .................................\n",
            "[CV] .................. n_neighbors=3, weights=distance, total=   0.0s\n",
            "[CV] n_neighbors=3, weights=distance .................................\n",
            "[CV] .................. n_neighbors=3, weights=distance, total=   0.0s\n",
            "[CV] n_neighbors=5, weights=uniform ..................................\n",
            "[CV] ................... n_neighbors=5, weights=uniform, total=   0.0s\n",
            "[CV] n_neighbors=5, weights=uniform ..................................\n",
            "[CV] ................... n_neighbors=5, weights=uniform, total=   0.0s\n",
            "[CV] n_neighbors=5, weights=uniform ..................................\n",
            "[CV] ................... n_neighbors=5, weights=uniform, total=   0.0s\n",
            "[CV] n_neighbors=5, weights=distance .................................\n",
            "[CV] .................. n_neighbors=5, weights=distance, total=   0.0s\n",
            "[CV] n_neighbors=5, weights=distance .................................\n",
            "[CV] .................. n_neighbors=5, weights=distance, total=   0.0s\n",
            "[CV] n_neighbors=5, weights=distance .................................\n",
            "[CV] .................. n_neighbors=5, weights=distance, total=   0.0s\n",
            "[CV] n_neighbors=7, weights=uniform ..................................\n",
            "[CV] ................... n_neighbors=7, weights=uniform, total=   0.0s\n",
            "[CV] n_neighbors=7, weights=uniform ..................................\n",
            "[CV] ................... n_neighbors=7, weights=uniform, total=   0.0s\n",
            "[CV] n_neighbors=7, weights=uniform ..................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] ................... n_neighbors=7, weights=uniform, total=   0.0s\n",
            "[CV] n_neighbors=7, weights=distance .................................\n",
            "[CV] .................. n_neighbors=7, weights=distance, total=   0.0s\n",
            "[CV] n_neighbors=7, weights=distance .................................\n",
            "[CV] .................. n_neighbors=7, weights=distance, total=   0.0s\n",
            "[CV] n_neighbors=7, weights=distance .................................\n",
            "[CV] .................. n_neighbors=7, weights=distance, total=   0.0s\n",
            "{'n_neighbors': 3, 'weights': 'uniform'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:    0.2s finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFmpyBZl3YlK",
        "outputId": "84aa3100-cd7b-4dac-8b0f-977d4b2052f4"
      },
      "source": [
        "# test model\n",
        "clf_knn_tuned =KNeighborsClassifier(n_neighbors=3, weights= \"uniform\")\n",
        "model = clf_knn_tuned.fit(X_train_knn,y_train)\n",
        "y_pred_knn = model.predict(X_test_knn)\n",
        "print(\"KNN Accuracy:\",metrics.accuracy_score(y_test, y_pred_knn))  # 0.9590"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Accuracy: 0.9590643274853801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERvb5nLibbf9",
        "outputId": "df4822bf-a17e-4064-b1ac-5bb7ec9e95bd"
      },
      "source": [
        "# svm\n",
        "kernel=['linear', 'sigmoid',  'rbf']\n",
        "C = [1, 5, 10]\n",
        "\n",
        "dic_grid = {'kernel': kernel, 'C': C}\n",
        "pprint(dic_grid)\n",
        "\n",
        "# Parameter tuning (takes long time)\n",
        "svm_grid = GridSearchCV(estimator = clf_svm, param_grid = dic_grid, \n",
        "                                    cv = 3, verbose=2)\n",
        "# Fit the grid search model\n",
        "X_new_svm_sc = StandardScaler().fit_transform(X_new_svm)\n",
        "svm_grid.fit(X_new_svm_sc, y)\n",
        "pprint(svm_grid.best_params_)\n",
        "\n",
        "# svm_grid.fit(X_train_svm, y)\n",
        "# pprint(svm_grid.best_params_)"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'C': [1, 5, 10], 'kernel': ['linear', 'sigmoid', 'rbf']}\n",
            "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
            "[CV] C=1, kernel=linear ..............................................\n",
            "[CV] ............................... C=1, kernel=linear, total=   0.0s\n",
            "[CV] C=1, kernel=linear ..............................................\n",
            "[CV] ............................... C=1, kernel=linear, total=   0.0s\n",
            "[CV] C=1, kernel=linear ..............................................\n",
            "[CV] ............................... C=1, kernel=linear, total=   0.0s\n",
            "[CV] C=1, kernel=sigmoid .............................................\n",
            "[CV] .............................. C=1, kernel=sigmoid, total=   0.0s\n",
            "[CV] C=1, kernel=sigmoid .............................................\n",
            "[CV] .............................. C=1, kernel=sigmoid, total=   0.0s\n",
            "[CV] C=1, kernel=sigmoid .............................................\n",
            "[CV] .............................. C=1, kernel=sigmoid, total=   0.0s\n",
            "[CV] C=1, kernel=rbf .................................................\n",
            "[CV] .................................. C=1, kernel=rbf, total=   0.0s\n",
            "[CV] C=1, kernel=rbf .................................................\n",
            "[CV] .................................. C=1, kernel=rbf, total=   0.0s\n",
            "[CV] C=1, kernel=rbf .................................................\n",
            "[CV] .................................. C=1, kernel=rbf, total=   0.0s\n",
            "[CV] C=5, kernel=linear ..............................................\n",
            "[CV] ............................... C=5, kernel=linear, total=   0.0s\n",
            "[CV] C=5, kernel=linear ..............................................\n",
            "[CV] ............................... C=5, kernel=linear, total=   0.0s\n",
            "[CV] C=5, kernel=linear ..............................................\n",
            "[CV] ............................... C=5, kernel=linear, total=   0.0s\n",
            "[CV] C=5, kernel=sigmoid .............................................\n",
            "[CV] .............................. C=5, kernel=sigmoid, total=   0.0s\n",
            "[CV] C=5, kernel=sigmoid .............................................\n",
            "[CV] .............................. C=5, kernel=sigmoid, total=   0.0s\n",
            "[CV] C=5, kernel=sigmoid .............................................\n",
            "[CV] .............................. C=5, kernel=sigmoid, total=   0.0s\n",
            "[CV] C=5, kernel=rbf .................................................\n",
            "[CV] .................................. C=5, kernel=rbf, total=   0.0s\n",
            "[CV] C=5, kernel=rbf .................................................\n",
            "[CV] .................................. C=5, kernel=rbf, total=   0.0s\n",
            "[CV] C=5, kernel=rbf .................................................\n",
            "[CV] .................................. C=5, kernel=rbf, total=   0.0s\n",
            "[CV] C=10, kernel=linear .............................................\n",
            "[CV] .............................. C=10, kernel=linear, total=   0.0s\n",
            "[CV] C=10, kernel=linear .............................................\n",
            "[CV] .............................. C=10, kernel=linear, total=   0.0s\n",
            "[CV] C=10, kernel=linear .............................................\n",
            "[CV] .............................. C=10, kernel=linear, total=   0.0s\n",
            "[CV] C=10, kernel=sigmoid ............................................\n",
            "[CV] ............................. C=10, kernel=sigmoid, total=   0.0s\n",
            "[CV] C=10, kernel=sigmoid ............................................\n",
            "[CV] ............................. C=10, kernel=sigmoid, total=   0.0s\n",
            "[CV] C=10, kernel=sigmoid ............................................\n",
            "[CV] ............................. C=10, kernel=sigmoid, total=   0.0s\n",
            "[CV] C=10, kernel=rbf ................................................\n",
            "[CV] ................................. C=10, kernel=rbf, total=   0.0s\n",
            "[CV] C=10, kernel=rbf ................................................\n",
            "[CV] ................................. C=10, kernel=rbf, total=   0.0s\n",
            "[CV] C=10, kernel=rbf ................................................\n",
            "[CV] ................................. C=10, kernel=rbf, total=   0.0s\n",
            "{'C': 1, 'kernel': 'rbf'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed:    0.1s finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5eT5XWXgvFZ",
        "outputId": "0c0969d1-58aa-42e8-9ac5-7b59c2793ba9"
      },
      "source": [
        "# test model\n",
        "clf_svm_tuned = svm.SVC(C=1, kernel= \"rbf\") # default parameter와 동일\n",
        "model = clf_svm_tuned.fit(X_train_svm,y_train)\n",
        "y_pred_svm = model.predict(X_test_svm)\n",
        "print(\"SVM Accuracy:\",metrics.accuracy_score(y_test, y_pred_svm))"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Accuracy: 0.9707602339181286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5FTIo5rhiJ1",
        "outputId": "636ed151-2bd1-49fc-f8b2-d9c206e5ac40"
      },
      "source": [
        "# Random Forest \n",
        "n_estimators = [100, 500, 1000, 1500, 2000]\n",
        "\n",
        "dic_grid = {'n_estimators': n_estimators}\n",
        "pprint(dic_grid)\n",
        "\n",
        "# Parameter tuning (takes long time)\n",
        "rf_grid = GridSearchCV(estimator = clf_rf, param_grid = dic_grid, \n",
        "                                   cv = 3, verbose=2)\n",
        "# Fit the random search model\n",
        "rf_grid.fit(X_new, y)\n",
        "pprint(rf_grid.best_params_)"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_estimators': [100, 500, 1000, 1500, 2000]}\n",
            "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
            "[CV] n_estimators=100 ................................................\n",
            "[CV] ................................. n_estimators=100, total=   0.2s\n",
            "[CV] n_estimators=100 ................................................\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] ................................. n_estimators=100, total=   0.2s\n",
            "[CV] n_estimators=100 ................................................\n",
            "[CV] ................................. n_estimators=100, total=   0.2s\n",
            "[CV] n_estimators=500 ................................................\n",
            "[CV] ................................. n_estimators=500, total=   0.9s\n",
            "[CV] n_estimators=500 ................................................\n",
            "[CV] ................................. n_estimators=500, total=   0.9s\n",
            "[CV] n_estimators=500 ................................................\n",
            "[CV] ................................. n_estimators=500, total=   0.9s\n",
            "[CV] n_estimators=1000 ...............................................\n",
            "[CV] ................................ n_estimators=1000, total=   1.8s\n",
            "[CV] n_estimators=1000 ...............................................\n",
            "[CV] ................................ n_estimators=1000, total=   1.9s\n",
            "[CV] n_estimators=1000 ...............................................\n",
            "[CV] ................................ n_estimators=1000, total=   1.8s\n",
            "[CV] n_estimators=1500 ...............................................\n",
            "[CV] ................................ n_estimators=1500, total=   2.7s\n",
            "[CV] n_estimators=1500 ...............................................\n",
            "[CV] ................................ n_estimators=1500, total=   2.7s\n",
            "[CV] n_estimators=1500 ...............................................\n",
            "[CV] ................................ n_estimators=1500, total=   2.7s\n",
            "[CV] n_estimators=2000 ...............................................\n",
            "[CV] ................................ n_estimators=2000, total=   3.5s\n",
            "[CV] n_estimators=2000 ...............................................\n",
            "[CV] ................................ n_estimators=2000, total=   3.6s\n",
            "[CV] n_estimators=2000 ...............................................\n",
            "[CV] ................................ n_estimators=2000, total=   3.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:   27.3s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_estimators': 500}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rAf3r4eh65U",
        "outputId": "d1e05515-cdb3-4ffb-e170-f8ca95d8d826"
      },
      "source": [
        "# Test tuned model #\n",
        "clf_rf_tuned = RandomForestClassifier(random_state=100,n_estimators=500)\n",
        "\n",
        "model = clf_rf_tuned.fit(X_train_rf,y_train)\n",
        "y_pred_rf = model.predict(X_test_rf)\n",
        "print(\"RF Accuracy:\",metrics.accuracy_score(y_test, y_pred_rf))  # 0.9649"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RF Accuracy: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rmJJRVdiLm-",
        "outputId": "58dbd79b-fd81-4296-9685-0b7f10b0c309"
      },
      "source": [
        "print(\"KNN Accuracy:\",metrics.accuracy_score(y_test, y_pred_knn))  # 0.9590\n",
        "print(\"SVM Accuracy:\",metrics.accuracy_score(y_test, y_pred_svm))\n",
        "print(\"RF Accuracy:\",metrics.accuracy_score(y_test, y_pred_rf))  # 0.9649"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Accuracy: 0.9590643274853801\n",
            "SVM Accuracy: 0.9707602339181286\n",
            "RF Accuracy: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dtDEcTNjlgj",
        "outputId": "a85b9de9-b9e3-4e7d-ba0f-dfbf0e265f1c"
      },
      "source": [
        "# Q5 : 구축한 3개의 모델을 hard voting 방식으로 앙상블하여 모델의 성능을 평가하시오.\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# hard voting classifier\n",
        "clf_voting = VotingClassifier(estimators=[('KNN', clf_knn_tuned),\n",
        "                                        ('SVM', clf_svm_tuned),\n",
        "                                        ('RF', clf_rf_tuned)],\n",
        "                              voting='hard')\n",
        "\n",
        "# Test voting classifer\n",
        "clf_voting.fit(X_train, y_train)\n",
        "pred_y = clf_voting.predict(X_test)\n",
        "print('Voting accuracy', metrics.accuracy_score(y_test, pred_y))\n"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Voting accuracy 0.9415204678362573\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwmAVaCdkwUT"
      },
      "source": [
        "# best model : random forest\n",
        "# 최고의 성능을 내면서 복잡하지 않은 모델"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}